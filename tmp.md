#### **算法**
- **输入**：训练数据集 $D$
- **输出**：CART 分类决策树
- **步骤**：
- 1. 从初始节点开始，选取特征 $A$，对该特征可取的每个值 $a$，将数据集 $D$ 分为两部分 $\{D_1|A=a\}$ 和 $\{D_2|A\ne a\}$，分别计算基尼指数，选择基尼指数最小的特征值作为该特征的最优切分点。
- 2. 对所有特征重复步骤 1，再在其中选取基尼指数最小的特征作为最优特征。根据最优特征及其最优切分点将数据集进行切分，把该节点分为两个子节点 $t_L$ 和 $t_R$，节点中的数据集即为 $D_1$ 和 $D_2$。
- 3. 对两个子节点递归地重复步骤 1 和步骤 2。
- 4. 当某一节点满足终止条件时，则停止切分，返回父节点，继续遍历其他节点。终止条件为节点中样本个数小于预定阈值，或节点中样本均为一个类别，或样本集的基尼指数小于预定阈值（样本基本属于同一类）。
- 5. 当所有节点均遍历之后，即生成 CART 分类决策树，结束算法。



#### **算法**
- **输入**：训练数据集 $D$
- **输出**：CART 回归决策树
- **步骤**：
- 1. 从初始节点开始，选取特征 $j$，遍历数据集 $D$ 中每个实例特征 $j$ 的取值 $s$，将数据集分为两部分$t_L=\{D_1|\mathbf x^{(j)}\le s\}$ 和 $t_R=\{D_2|\mathbf x^{(j)}\ge s\}$，每部分的输出值即为该部分所有实例的平均值 $\overline{y}_t$，依次用 $\displaystyle \sum_{x_i \in t_L} (y_i-\overline{y}_{t_L})^2+ \sum_{x_i \in t_R} (y_i-\overline{y}_{t_R})^2$ 计算分类误差，误差最小的 $s$ 即为最优切分值。
- 2. 对所有特征重复步骤 1，再在其中选取分类误差最小的特征作为最优特征。根据最优特征及其最优切分点将数据集进行切分，把该节点分为两个子节点 $t_L$ 和 $t_R$，节点中的数据集即为 $\{D_1|\mathbf x^{(j)}\le s\}$ 和 $\{D_2|\mathbf x^{(j)}\ge s\}$
- 3. 对两个子节点递归地重复步骤 1 和步骤 2。
- 4. 当某一节点满足终止条件时，则停止切分，返回父节点，继续遍历其他节点。终止条件为节点中的样本个数小于特定阈值（通常为5），或者节点中样本均为一个类别，即该节点为纯节点。
- 5. 当所有节点均遍历之后，即生成 CART 回归决策树，结束算法。