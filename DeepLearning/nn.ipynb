{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = [\n",
    "    # nn.Sigmoid,\n",
    "    # nn.Tanh,\n",
    "    # nn.Hardsigmoid,\n",
    "    # nn.Hardtanh,\n",
    "    nn.ReLU,\n",
    "    # nn.LeakyReLU,\n",
    "    # nn.ELU,\n",
    "    # nn.Softplus,\n",
    "    # nn.GELU,\n",
    "]\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, activation):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            activation(),\n",
    "            nn.Linear(512, 512),\n",
    "            activation(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "models = [NeuralNetwork(activ) for activ in activations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.253252  [    0/60000]   [32/64]\n",
      "loss: 2.246472  [ 6400/60000]   [34/64]\n",
      "loss: 2.248913  [12800/60000]   [35/64]\n",
      "loss: 2.225269  [19200/60000]   [34/64]\n",
      "loss: 2.225507  [25600/60000]   [43/64]\n",
      "loss: 2.223863  [32000/60000]   [33/64]\n",
      "loss: 2.195054  [38400/60000]   [43/64]\n",
      "loss: 2.229855  [44800/60000]   [34/64]\n",
      "loss: 2.196735  [51200/60000]   [35/64]\n",
      "loss: 2.179539  [57600/60000]   [47/64]\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 2.186033 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.189488  [    0/60000]   [42/64]\n",
      "loss: 2.175921  [ 6400/60000]   [42/64]\n",
      "loss: 2.186706  [12800/60000]   [38/64]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-ae4476e4a80c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {t+1}\\n-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-ae4476e4a80c>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# Compute prediction and loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-a828c7b1708f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_relu_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": "<Figure size 432x288 with 1 Axes>"
     },
     "metadata": {
      "needs_background": "light",
      "transient": {}
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        \n",
    "        loss = loss_fn(pred, y)\n",
    "        train_loss += loss.item()\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        out_t = pred.argmax(dim=1) #取出预测的最大值\n",
    "        num_correct = (out_t == y).sum().item()\n",
    "        train_acc += num_correct\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]   [{num_correct}/{out_t.shape[0]}]\")\n",
    "    \n",
    "    return train_loss / len(dataloader), train_acc / len(dataloader.dataset)\n",
    "        \n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss, correct\n",
    "\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel('x label')  # Add an x-label to the axes.\n",
    "ax.set_ylabel('y label')  # Add a y-label to the axes.\n",
    "ax.set_title(\"Simple Plot\")  # Add a title to the axes.\n",
    "\n",
    "for model in models:\n",
    "    train_losses = []\n",
    "    train_acces = []\n",
    "    test_losses = []\n",
    "    test_acces = []  \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loss, train_acc = train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "        test_loss, test_acc = test_loop(test_dataloader, model, loss_fn)\n",
    "        train_losses.append(train_loss)\n",
    "        train_acces.append(train_acc)\n",
    "        test_losses.append(test_loss)\n",
    "        test_acces.append(test_acc)\n",
    "        ax.plot(np.arange(len(train_losses)), train_losses, label='sigmoid')\n",
    "\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.292503  [    0/60000]\n",
      "loss: 2.296124  [ 6400/60000]\n",
      "loss: 2.287753  [12800/60000]\n",
      "loss: 2.272686  [19200/60000]\n",
      "loss: 2.273002  [25600/60000]\n",
      "loss: 2.273609  [32000/60000]\n",
      "loss: 2.262012  [38400/60000]\n",
      "loss: 2.275711  [44800/60000]\n",
      "loss: 2.252830  [51200/60000]\n",
      "loss: 2.245801  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 53.6%, Avg loss: 2.245907 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.241593  [    0/60000]\n",
      "loss: 2.240510  [ 6400/60000]\n",
      "loss: 2.247666  [12800/60000]\n",
      "loss: 2.205970  [19200/60000]\n",
      "loss: 2.218712  [25600/60000]\n",
      "loss: 2.216462  [32000/60000]\n",
      "loss: 2.195923  [38400/60000]\n",
      "loss: 2.220791  [44800/60000]\n",
      "loss: 2.182974  [51200/60000]\n",
      "loss: 2.177792  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.9%, Avg loss: 2.171800 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.165527  [    0/60000]\n",
      "loss: 2.154490  [ 6400/60000]\n",
      "loss: 2.184784  [12800/60000]\n",
      "loss: 2.100370  [19200/60000]\n",
      "loss: 2.130687  [25600/60000]\n",
      "loss: 2.121399  [32000/60000]\n",
      "loss: 2.085382  [38400/60000]\n",
      "loss: 2.126260  [44800/60000]\n",
      "loss: 2.063762  [51200/60000]\n",
      "loss: 2.058409  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 2.043046 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.034038  [    0/60000]\n",
      "loss: 2.006305  [ 6400/60000]\n",
      "loss: 2.070562  [12800/60000]\n",
      "loss: 1.924247  [19200/60000]\n",
      "loss: 1.972978  [25600/60000]\n",
      "loss: 1.952797  [32000/60000]\n",
      "loss: 1.898904  [38400/60000]\n",
      "loss: 1.964916  [44800/60000]\n",
      "loss: 1.863064  [51200/60000]\n",
      "loss: 1.859413  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.9%, Avg loss: 1.828409 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.824531  [    0/60000]\n",
      "loss: 1.769544  [ 6400/60000]\n",
      "loss: 1.867970  [12800/60000]\n",
      "loss: 1.661884  [19200/60000]\n",
      "loss: 1.710862  [25600/60000]\n",
      "loss: 1.679894  [32000/60000]\n",
      "loss: 1.622502  [38400/60000]\n",
      "loss: 1.720600  [44800/60000]\n",
      "loss: 1.575909  [51200/60000]\n",
      "loss: 1.573103  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 1.525178 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.546013  [    0/60000]\n",
      "loss: 1.452435  [ 6400/60000]\n",
      "loss: 1.566610  [12800/60000]\n",
      "loss: 1.347153  [19200/60000]\n",
      "loss: 1.374148  [25600/60000]\n",
      "loss: 1.340230  [32000/60000]\n",
      "loss: 1.294637  [38400/60000]\n",
      "loss: 1.425276  [44800/60000]\n",
      "loss: 1.274618  [51200/60000]\n",
      "loss: 1.259368  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.2%, Avg loss: 1.208427 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.264727  [    0/60000]\n",
      "loss: 1.142808  [ 6400/60000]\n",
      "loss: 1.247638  [12800/60000]\n",
      "loss: 1.072641  [19200/60000]\n",
      "loss: 1.076758  [25600/60000]\n",
      "loss: 1.045192  [32000/60000]\n",
      "loss: 1.013728  [38400/60000]\n",
      "loss: 1.165313  [44800/60000]\n",
      "loss: 1.044617  [51200/60000]\n",
      "loss: 1.016278  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.969005 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.049964  [    0/60000]\n",
      "loss: 0.919426  [ 6400/60000]\n",
      "loss: 1.005331  [12800/60000]\n",
      "loss: 0.883173  [19200/60000]\n",
      "loss: 0.874932  [25600/60000]\n",
      "loss: 0.845187  [32000/60000]\n",
      "loss: 0.820677  [38400/60000]\n",
      "loss: 0.980364  [44800/60000]\n",
      "loss: 0.891242  [51200/60000]\n",
      "loss: 0.858931  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Avg loss: 0.811098 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.902680  [    0/60000]\n",
      "loss: 0.771909  [ 6400/60000]\n",
      "loss: 0.841451  [12800/60000]\n",
      "loss: 0.758921  [19200/60000]\n",
      "loss: 0.743087  [25600/60000]\n",
      "loss: 0.716743  [32000/60000]\n",
      "loss: 0.690740  [38400/60000]\n",
      "loss: 0.854097  [44800/60000]\n",
      "loss: 0.787227  [51200/60000]\n",
      "loss: 0.757744  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.705846 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.800060  [    0/60000]\n",
      "loss: 0.670352  [ 6400/60000]\n",
      "loss: 0.728359  [12800/60000]\n",
      "loss: 0.675600  [19200/60000]\n",
      "loss: 0.652019  [25600/60000]\n",
      "loss: 0.631601  [32000/60000]\n",
      "loss: 0.599137  [38400/60000]\n",
      "loss: 0.766318  [44800/60000]\n",
      "loss: 0.712264  [51200/60000]\n",
      "loss: 0.689802  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.3%, Avg loss: 0.632219 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()\n",
    "\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}